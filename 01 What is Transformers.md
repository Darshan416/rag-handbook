# Exploring Transformers: A Journey from OCR to Deep Learning

A couple of years ago, while experimenting with open-source OCR models for handwriting recognition, I came across TrOCR (Transformer-based OCR). Though I ultimately deployed Document Intelligence Studio at an enterprise scale, this exploration led me down a fascinating rabbit holeâ€”the world of Transformers.

Diving deep into Transformer architecture was a game-changer, helping me understand the backbone of modern NLP and generative AI models. Sharing some of the best resources that helped me grasp the fundamentals and practical aspects of Transformers:

## ðŸ“Œ The Paper That Started It All

ðŸ”— Attention Is All You Need (Vaswani et al., 2017) -[https://lnkd.in/gcq88STh](https://lnkd.in/gcq88STh)

## ðŸ“Œ Harvard NLP's Annotated Transformer

ðŸ”— Attention and Transformer Models -[https://lnkd.in/g6x56eHk](https://lnkd.in/g6x56eHk)

## ðŸ“Œ Intuitive Visual Explanations

ðŸ”— Jay Alammarâ€™s Illustrated Transformer -[https://lnkd.in/gde5wuXV](https://lnkd.in/gde5wuXV)
ðŸ”— Visualizing Attention in Seq2Seq Models -[https://lnkd.in/g8WdYinN](https://lnkd.in/g8WdYinN)

## ðŸ“Œ Detailed Video Explanations

ðŸŽ¥ Umar Jamilâ€™s Breakdown of Transformer Architecture -[https://lnkd.in/gYEPqUvQ](https://lnkd.in/gYEPqUvQ)
ðŸŽ¥ Coding Transformers from Scratch -[https://lnkd.in/gkZdFfxV](https://lnkd.in/gkZdFfxV)

## ðŸ“Œ Hands-On Learning

ðŸŽ“[DeepLearning.AI](http://deeplearning.ai/) Short Course on Transformers & LLMs -[https://lnkd.in/gzFkJDyM](https://lnkd.in/gzFkJDyM)

For anyone looking to build a solid foundation in Transformers, self-attention mechanisms, and Large Language Models (LLMs), these resources are invaluable.
