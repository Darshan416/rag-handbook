# **ğŸ§ª How to evaluate RAG systems?**

In Retrieval-Augmented Generation (RAG) systems, the retriever is the gatekeeper of relevance. It decides what information your LLM will see. So before you tune the generator, you must ask:

## ğŸ‘‰ Is my retriever surfacing the right documents?

Letâ€™s explore how to measure retrieval quality beyond just latency or throughput. Weâ€™re talking about precision, recall, MAP, and MRRâ€”the true north metrics that tell you whether your retriever is doing its job.

## ğŸ¯ Start with Ground Truth

To evaluate a retriever properly, you need three ingredients:

1) A prompt (the user query)
2) The retrieverâ€™s ranked results
3) A ground truth list of relevant documents for that prompt
   This ground truth is criticalâ€”itâ€™s your "answer key." Without it, you're guessing.

## ğŸ“ Precision vs. Recall

These are the core building blocks of retrieval evaluation:
Precision = Relevant / Retrieved
â†’ Measures trustworthiness. Are the results useful?
Recall = Relevant / Relevant in corpus
â†’ Measures coverage. Are you missing anything important?
ğŸ’¡ Trade-off: Increase recall â†’ risk more irrelevant docs â†’ precision drops. The reverse is also true.

## ğŸ“¦ Precision@K and Recall@K

Since you rarely show users 100 results, we often measure these within the top K results:
Precision@10: % of top 10 that are relevant
Recall@10: % of all relevant docs captured in top 10
ğŸ“Š Changing K helps simulate strict vs. lenient evaluation environments.

## â­ Mean Average Precision (MAP@K)

MAP looks beyond just how many relevant docs were retrievedâ€”it cares about where they were ranked.
Penalizes relevant documents ranked low
Rewards for putting relevant docs higher
Useful when you want to rank well, not just retrieve

## ğŸ” Mean Reciprocal Rank (MRR)

Sometimes, getting just one relevant doc fast is more important than getting all of them.
Reciprocal Rank = 1 / (rank of first relevant doc)
MRR = average across multiple prompts
Great when you want to optimize for the first relevant hitâ€”e.g., in FAQs, support bots, or QA systems.

## âš™ï¸ Why It Matters in RAG

Every time your retriever surfaces irrelevant docs:

1) Your LLM may hallucinate
2) You waste tokens and context space
3) User trust drops

## By tracking precision, recall, MAP, and MRR:

âœ… You catch issues early
âœ… You validate hybrid retrieval setups (e.g., BM25 + dense embeddings)
âœ… You optimize for relevance before worrying about generation quality

## ğŸš¨ Caveat:

All these metrics need labeled dataâ€”a list of what should be retrieved. That can be manual and time-consuming. But once you have it, you can:
Compare retriever configs
A/B test hybrid strategies
Monitor quality drift in production
In RAG systems, retrieval is the foundation. If your retriever fails, your generator never had a chance.

ğŸ“Œ TL;DR: Evaluate it. Measure it. Tune it. Your LLM will thank you later.
